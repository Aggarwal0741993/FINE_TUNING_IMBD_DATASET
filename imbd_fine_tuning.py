# -*- coding: utf-8 -*-
"""imbd_fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LkxHBKInN-zakOL8t3UASvUuxkk5WBxK
"""

!pip install -U transformers datasets evaluate fsspec

from transformers import AutoTokenizer,TrainingArguments, Trainer,AutoModelForSequenceClassification,EarlyStoppingCallback
from datasets import load_dataset
import evaluate
import numpy as np

import kagglehub

# Download latest version
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")

print("Path to dataset files:", path)

import os
print(os.listdir('/kaggle/input/imdb-dataset-of-50k-movie-reviews'))

from datasets import load_dataset, DatasetDict
import os

# Use the path obtained from kagglehub.dataset_download
csv_file_path = os.path.join(path, "IMDB Dataset.csv")

# Check if the file exists and has content before loading
if not os.path.exists(csv_file_path):
    raise FileNotFoundError(f"The file was not found at: {csv_file_path}")

# You might want to add a check for file size or read a small part to ensure it's not empty

try:
    dataset = load_dataset("csv", data_files=csv_file_path)
except Exception as e:
    print(f"An error occurred while loading the dataset: {e}")
    print("Please check the file content and format.")

dataset

from sklearn.model_selection import train_test_split
train_test=dataset["train"].train_test_split(test_size=0.2)
train_test

test_dataset=train_test["test"]
train_dataset_temp=train_test["train"]

train_valid=train_dataset_temp.train_test_split(test_size=0.4)
train_dataset=train_valid["train"]
valid_dataset=train_valid["test"]

dataset=DatasetDict({"train":train_dataset,"test":test_dataset,"valid":valid_dataset})
dataset

# Load model directly
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer=AutoTokenizer.from_pretrained("roberta-base")
model = AutoModelForSequenceClassification.from_pretrained("roberta-base",num_labels=2)

def tokenize_function(examples):
    result = tokenizer(examples["review"], padding="max_length", truncation=True,max_length=128)
    result["labels"] = [1 if s == "positive" else 0 for s in examples["sentiment"]] # Convert sentiment to 0 or 1
    return result

tokenized_dataset=dataset.map(tokenize_function,batched=True)

tokenized_dataset=tokenized_dataset.remove_columns(["review", "sentiment"])

tokenized_dataset

tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
tokenized_dataset

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
def compute_metrics(eva_pred):
  logits, labels = eva_pred
  predict = np.argmax(logits, axis=1)
  precision, recall, f1, _ = precision_recall_fscore_support(labels, predict, average='binary')
  acc = accuracy_score(labels, predict)
  return {
      'accuracy': acc,
      'f1': f1,
      'precision': precision,
      'recall': recall
  }

#tokenized_dataset['train']=tokenized_dataset["train"].select(range(10000))
#tokenized_dataset['valid']=tokenized_dataset["test"].select(range(1000))

from transformers import TrainingArguments

training_args=TrainingArguments(output_dir="./results",eval_strategy="epoch",save_strategy="epoch",learning_rate=2e-5,
                                per_device_train_batch_size=8,per_device_eval_batch_size=8,num_train_epochs=2,
                                weight_decay=0.03,logging_dir="./logs",logging_steps=50, report_to="none",
                                load_best_model_at_end=True,
                                greater_is_better=False,
                                metric_for_best_model="eval_loss")

trainer=Trainer(model=model,args=training_args,train_dataset=tokenized_dataset["train"],
                eval_dataset=tokenized_dataset["valid"],compute_metrics=compute_metrics,
                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])

trainer.train()

trainer.evaluate(eval_dataset=tokenized_dataset["test"])

trainer.save_model("imbd_fine_tuned")
tokenizer.save_pretrained("imbd_fine_tuned")

import shutil
shutil.make_archive("imbd_fine_tuned", 'zip', "imbd_fine_tuned")

from google.colab import files
files.download('imbd_fine_tuned.zip')